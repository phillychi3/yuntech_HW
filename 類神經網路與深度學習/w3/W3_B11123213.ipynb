{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[線上筆記連結](https://app.heptabase.com/w/7ec6ef5bbaf4b1363f78e04e669381dcd754a217f1cb1a207c0a73354a984e2f)  \n",
    "pytorch 裡面用了很多非 python 的 func，例如使用 c++與 cuda，這樣可以在 GPU 上以較高效率的運行\n",
    "\n",
    "PyTorch 有了具 autograd 功能的張量函式庫\n",
    "autograd 的意思是自動求導，簡單說就是~~自動微分~~\n",
    "\n",
    "可以使用多個伺服器與 GPU 去訓練模型，這樣可以提高訓練的效率\n",
    "\n",
    "為了訓練模型，我們需要一組訓練資料與一個優化器\n",
    "\n",
    "## touch.nn 元件\n",
    "\n",
    "**層**\n",
    "\n",
    "- 線性層（Linear）\n",
    "\n",
    "- 卷積層（Conv2d）\n",
    "\n",
    "- 池化層（MaxPool2d）\n",
    "\n",
    "- 激活函數（ReLU）\n",
    "\n",
    "- 歸一化層（BatchNorm2d）\n",
    "\n",
    "- 遞歸層（LSTM）\n",
    "\n",
    "- 嵌入層（Embedding）\n",
    "\n",
    "**損失函數**\n",
    "\n",
    "- 均方誤差（MSELoss）\n",
    "\n",
    "- 交叉熵（CrossEntropyLoss）\n",
    "\n",
    "- L1 損失（L1Loss）\n",
    "\n",
    "- SmoothL1Loss\n",
    "\n",
    "**優化器**\n",
    "\n",
    "- 梯度下降（SGD）\n",
    "\n",
    "- Adam\n",
    "\n",
    "- RMSProp\n",
    "\n",
    "- AdaGrad\n",
    "\n",
    "## 優化器是什麼?\n",
    "\n",
    "優化器 Optimizer 是機器學習中用於調整模型參數以最小化損失函數的算法。\n",
    "\n",
    "優化器的主要作用是：\n",
    "\n",
    "- 加速模型的訓練速度\n",
    "- 提高模型的準確性\n",
    "- 防止模型陷入局部極小值\n",
    "\n",
    "## 分布式訓練\n",
    "\n",
    "為了加快模型訓練速度，可以使用多個伺服器和 GPU 進行分布式訓練。\n",
    "\n",
    "torch.nn.parallel.DistributedDataParallel 和 torch.distributed 可以進行分散式訓練，運作方式為：\n",
    "\n",
    "- torch.nn.parallel.DistributedDataParallel 將一個模型複製到多個 GPU 上，並在每個 GPU 上獨立地進行正向和反向傳播。\n",
    "- 使用 torch.distributed 的同步機制進行梯度同步，最後更新模型參數。\n",
    "\n",
    "## 效能損耗\n",
    "\n",
    "每當 Python 執行 PyTorch 的指令時，相應的操作會由 C++ 或 CUDA 執行。當更多的指令在張量上執行時，後端也就需要執行更多的操作。\n",
    "雖然 C++ 執行速度很快，但透過 Python 來呼叫 C++ 函式時，會產生一些效能損耗。\n",
    "\n",
    "## CUDA\n",
    "\n",
    "torch.cuda.is_available()來查看目前設備是否有支援CUDA運算\n",
    "如果沒有GPU可以在google colab裡面變更執行階段類型，改成T4 GPU，即可使用cuda的功能\n",
    "\n",
    "## TPU\n",
    "\n",
    "TPU( Tensor Processing Unit)，是 Google 研發用於機器學習的加速器。TPU 可以提高機器學習模型的訓練和推理速度，並降低成本，並且目前比a100的效能更高，價格更低。\n",
    "\n",
    "TPU 採用了專門為機器學習設計的硬件架構，可以高效地執行機器學習模型中的各種操作，例如矩陣乘法、卷積和池化等。\n",
    "\n",
    "為了比GPU更快，google設計了TPU，TPU為了專注於矩陣運算直接設計了數千個乘法器和加法器直連的大型物理矩陣。\n",
    "\n",
    "並且在計算過程中，不須請求記憶體\n",
    "\n",
    "可以使用`torch-tpu`來為pytorch使用tpu\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在圖像處理中，張量的第一維度通常表示圖像的數量，第二維度表示圖像的高度，第三維度表示圖像的寬度。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ctrl + enter 執行  \n",
    "ctrl + M B 向上增加  \n",
    "ctrl + M A 向下增加  \n",
    "ctrl + M D 刪除  \n",
    "ctrl + m z 復原"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
